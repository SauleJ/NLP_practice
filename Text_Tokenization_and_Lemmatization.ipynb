{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python script demonstrates various text processing tasks using different libraries such as SpaCy and NLTK. It includes examples of tokenization, sentence segmentation, lemmatization, and stemming, providing insights into the different approaches and tools available for handling textual data.\n",
    "\n",
    "1. Tokenization with SpaCy:\n",
    "   Utilizes the SpaCy library to tokenize the text and count the total number of words and punctuation marks.\n",
    "\n",
    "2. Tokenization with NLTK:\n",
    "   Demonstrates tokenization using the NLTK library, calculating the word and punctuation mark counts in the text.\n",
    "\n",
    "3. Sentence Segmentation with SpaCy:\n",
    "   Uses SpaCy to segment the text into sentences and counts the total number of sentences.\n",
    "\n",
    "4. Sentence Segmentation with NLTK:\n",
    "   Applies NLTK for sentence segmentation and calculates the total number of sentences in the text.\n",
    "\n",
    "5. Printing the Fourth Sentence:\n",
    "   Prints the fourth sentence from the provided text.\n",
    "\n",
    "6. Tokenizing and Stemming the Fourth Sentence:\n",
    "   Tokenizes the fourth sentence into words and punctuation marks and then applies stemming to the words using NLTK.\n",
    "\n",
    "7. Lemmatizing the Words of the Fourth Sentence:\n",
    "   Lemmatizes the words of the fourth sentence using SpaCy, obtaining their base forms.\n",
    "\n",
    "8. Finding Lemma Forms for Words in the fourth Sentence:\n",
    "   Identifies the lemma forms of words in the fourth sentence using SpaCy.\n",
    "\n",
    "9. Tokenizing the First Sentence of Lithuanian Text:\n",
    "   Tokenizes the first sentence of the Lithuanian text into words and punctuation marks using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "text=\"Welcome to Vilnius University – the oldest and largest Lithuanian higher education institution. Since its establishment in the 16th century, Vilnius University, as integral part of European science and culture has embodied the concept­ of a classical university and the unity of studies and research. Vilnius University is an active participant in international scientific and academic activities and boasts many prominent scientists, professors and graduates. Scientific development and the expanding relations with global research centres have contributed to the variety of research and studies at Vilnius University. With the support of social partners, the university educates globally–minded specialists who successfully integrate in the modern European community.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting text into text units (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How many words and punctuation marks are there in the text?\n",
    "Calculate using the SpaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_text = nlp(text)\n",
    "\n",
    "word_number = len([token for token in nlp_text if not token.is_punct])\n",
    "punctuation_sign_number = len([token for token in nlp_text if token.is_punct])\n",
    "\n",
    "print(word_number + punctuation_sign_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How many words and punctuation marks are there in the text?\n",
    "Compute using the NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of words: 115\n",
      "Number of punctuation marks: 13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "word_number = len(words)\n",
    "\n",
    "punctuation_sign_number = len([token for token in words if not token.isalpha()])\n",
    "\n",
    "print(f\"Numbers of words: {word_number}\")\n",
    "print(f\"Number of punctuation marks: {punctuation_sign_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How many sentences are there in the text?\n",
    "Calculate using the SpaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp_text = nlp(text)\n",
    "\n",
    "number_sentences = len(list(nlp_text.sents))\n",
    "\n",
    "print(f\"Number of sentences: {number_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. How many sentences are there in the text?\n",
    "Compute using the NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentence_count = len(sentences)\n",
    "\n",
    "print(f\"Number of sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Print the fourth sentence of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourth sentence: Scientific development and the expanding relations with global research centres have contributed to the variety of research and studies at Vilnius University.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "fourth_sentence = sentences[3]\n",
    "print(f\"Fourth sentence: {fourth_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Break the fourth sentence into words and punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split fourth sentence: ['Scientific', 'development', 'and', 'the', 'expanding', 'relations', 'with', 'global', 'research', 'centres', 'have', 'contributed', 'to', 'the', 'variety', 'of', 'research', 'and', 'studies', 'at', 'Vilnius', 'University', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "fourth_sentence = sentences[3]\n",
    "words_and_punctuation = nltk.word_tokenize(fourth_sentence)\n",
    "\n",
    "print(f\"Split fourth sentence: {words_and_punctuation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Lemmatizing the Words of the Fourth Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: ['scientific', 'development', 'and', 'the', 'expand', 'relation', 'with', 'global', 'research', 'centre', 'have', 'contribute', 'to', 'the', 'variety', 'of', 'research', 'and', 'study', 'at', 'Vilnius', 'University', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "fourth_sentence = sentences[3]\n",
    "nlp_fourth_sentence = nlp(fourth_sentence)\n",
    "\n",
    "lemmas = [token.lemma_ for token in nlp_fourth_sentence]\n",
    "print(f\"Lemmas: {lemmas}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Finding Lemma Forms for Words in the fourth Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: ['scientif', 'develop', 'and', 'the', 'expand', 'relat', 'with', 'global', 'research', 'centr', 'have', 'contribut', 'to', 'the', 'varieti', 'of', 'research', 'and', 'studi', 'at', 'vilniu', 'univers', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "fourth_sentence = sentences[3]\n",
    "\n",
    "doc = nlp(fourth_sentence)\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stems = [porter_stemmer.stem(word) for word in lemmas]\n",
    "\n",
    "print(f\"Stems: {stems}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Lithuanian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"Vilniaus universitetas (VU) – pirmasis ir didžiausias Lietuvos universitetas, įsikūręs šalies sostinėje Vilniuje ir turintis po padalinį Kaune ir Šiauliuose. Įkurtas 1579 m. VU yra šalies lyderis absoliučioje daugumoje mokslo ir studijų krypčių. Būdamas viena seniausių ir žymiausių Vidurio ir Rytų Europos aukštųjų mokyklų, VU darė didelę įtaką ne tik Lietuvos, bet ir kaimyninių šalių kultūriniam gyvenimui, išugdė ne vieną mokslininkų, poetų, kultūros veikėjų kartą. VU profesoriavo ir mokėsi daug garsių asmenybių: medikai vokiečiai Johanas Frankas ir jo sūnus Jozefas Frankas, istorikas Joachimas Lelevelis, poetai Adomas Mickevičius ir Julius Slovackis, istorikas Simonas Daukantas, rašytojas, poetas ir literatūros mokslininkas Česlovas Milošas. VU įkurtas sklindant renesanso, reformacijos ir katalikiškosios reformos idėjoms. Už VU anksčiau Europoje įkurti tik Prahos, Krokuvos, Pečo, Budapešto, Bratislavos ir Karaliaučiaus universitetai.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Tokenizing the First Sentence of Lithuanian Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized first sentence: ['Vilniaus', 'universitetas', '(', 'VU', ')', '–', 'pirmasis', 'ir', 'didžiausias', 'Lietuvos', 'universitetas', ',', 'įsikūręs', 'šalies', 'sostinėje', 'Vilniuje', 'ir', 'turintis', 'po', 'padalinį', 'Kaune', 'ir', 'Šiauliuose', '.']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "sentences = nltk.sent_tokenize(text2)\n",
    "\n",
    "first_sentence = sentences[0]\n",
    "\n",
    "words_and_punctuation = nltk.word_tokenize(first_sentence)\n",
    "print(f\"Tokenized first sentence: {words_and_punctuation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
